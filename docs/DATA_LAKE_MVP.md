# Data Lake MVP - æœ€å°å¯è¡Œç‰ˆæœ¬

## ğŸ¯ æ ¸å¿ƒç›®æ ‡

**ä¸€å¥è¯æè¿°ï¼š**
ä»é‡‡é›†èŠ‚ç‚¹ï¼ˆé€šè¿‡ SSH/Tailscaleï¼‰ç¨³å®šåŒæ­¥ Parquet diff æ–‡ä»¶åˆ°æœ¬åœ°ï¼ŒæŒ‰é…ç½®ç®¡ç†ä¿ç•™æœŸï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ã€‚

**ä¸åšçš„äº‹ï¼š**
- âŒ ä¸åˆå¹¶ diff æ–‡ä»¶ï¼ˆä¿æŒ part_xxxxx.parquet åŸå§‹ç»“æ„ï¼‰
- âŒ ä¸åšæ•°æ®è½¬æ¢
- âŒ ä¸åšå®æ—¶è®¢é˜…
- âŒ ä¸åšæ•°æ®éªŒè¯ï¼ˆå‡è®¾æºç«¯å·²éªŒè¯ï¼‰

**åªåšçš„äº‹ï¼š**
- âœ… rsync åŒæ­¥è¿œç¨‹ç›®å½•
- âœ… æ–­ç‚¹ç»­ä¼ ï¼ˆ`--partial --inplace`ï¼‰
- âœ… checkpoint è®°å½•åŒæ­¥çŠ¶æ€
- âœ… retention è‡ªåŠ¨æ¸…ç†æ—§æ•°æ®

---

## ğŸ“‹ é…ç½® Schema

### æœ€å°é…ç½®ç¤ºä¾‹

```yaml
# config/data_lake.yml
data_lake:
  # å…¨å±€è®¾ç½®
  root_dir: /data/lake                    # æœ¬åœ° Data Lake æ ¹ç›®å½•
  checkpoint_dir: /data/lake/.checkpoints # checkpoint æ–‡ä»¶ç›®å½•ï¼ˆå¯é€‰ï¼‰

  # æ•°æ®æºé…ç½®ï¼ˆprofilesï¼‰
  profiles:
    # Profile 1: CEX Orderbook Ticks
    cex_ticks:
      enabled: true

      # è¿œç¨‹æ•°æ®æº
      source:
        type: ssh                         # ä»…æ”¯æŒ "ssh"
        host: 10.0.0.11                   # Tailscale IP æˆ–å…¬ç½‘ IP
        port: 6677                        # SSH ç«¯å£ï¼ˆé»˜è®¤ 6677ï¼‰
        user: ubuntu
        ssh_key: ~/.ssh/lightsail_key.pem # SSH ç§é’¥è·¯å¾„
        remote_root: /var/data/cex_tickers # è¿œç¨‹æ•°æ®ç›®å½•

      # æœ¬åœ°å­˜å‚¨
      local_subdir: cex_ticks             # æœ¬åœ°å­ç›®å½•ï¼ˆç›¸å¯¹ root_dirï¼‰

      # ä¿ç•™ç­–ç•¥
      retention_days: 30                  # ä¿ç•™ 30 å¤©æ•°æ®

      # åŒæ­¥é€‰é¡¹
      rsync_args: "-az --partial --inplace --delete"
      # -a: archive mode (ä¿ç•™æƒé™/æ—¶é—´æˆ³)
      # -z: å‹ç¼©ä¼ è¾“
      # --partial: æ–­ç‚¹ç»­ä¼ 
      # --inplace: å°±åœ°æ›´æ–°ï¼ˆé¿å…ä¸´æ—¶æ–‡ä»¶ï¼‰
      # --delete: åˆ é™¤è¿œç«¯å·²åˆ é™¤çš„æ–‡ä»¶

      # Checkpointï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰
      checkpoint_file: /data/lake/.checkpoints/cex_ticks.json

    # Profile 2: DEX OHLCV
    dex_candles:
      enabled: false  # æš‚æ—¶ç¦ç”¨
      source:
        type: ssh
        host: 10.0.0.12
        user: ubuntu
        remote_root: /var/data/dex_candles
      local_subdir: dex_candles
      retention_days: 60
      rsync_args: "-az --partial"
```

### Schema éªŒè¯è§„åˆ™

**å¿…éœ€å­—æ®µï¼š**
- `data_lake.root_dir`
- `profiles.<name>.source.type` (å¿…é¡»æ˜¯ "ssh")
- `profiles.<name>.source.host`
- `profiles.<name>.source.user`
- `profiles.<name>.source.remote_root`
- `profiles.<name>.local_subdir`

**å¯é€‰å­—æ®µï¼š**
- `source.port` (é»˜è®¤: 6677)
- `source.ssh_key` (é»˜è®¤: ~/.ssh/lightsail_key.pem)
- `retention_days` (é»˜è®¤: 30)
- `rsync_args` (é»˜è®¤: "-az --partial --inplace")
- `checkpoint_file` (é»˜è®¤: `{checkpoint_dir}/{profile_name}.json`)

---

## ğŸ”§ CLI å‘½ä»¤

### å‘½ä»¤ 1: åŒæ­¥æ•°æ®

```bash
# åŒæ­¥å•ä¸ª profile
quants-infra data-lake sync cex_ticks

# åŒæ­¥æ‰€æœ‰å¯ç”¨çš„ profiles
quants-infra data-lake sync --all

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
quants-infra data-lake sync cex_ticks --config config/data_lake.yml

# å¹²è·‘ï¼ˆä»…æ˜¾ç¤ºå°†è¦æ‰§è¡Œçš„æ“ä½œï¼‰
quants-infra data-lake sync cex_ticks --dry-run
```

**æ‰§è¡Œæµç¨‹ï¼š**
```
1. åŠ è½½é…ç½®æ–‡ä»¶
2. éªŒè¯é…ç½®ï¼ˆschema validationï¼‰
3. æ£€æŸ¥ checkpointï¼ˆä¸Šæ¬¡åŒæ­¥æ—¶é—´ï¼‰
4. æ„å»º rsync å‘½ä»¤
5. æ‰§è¡Œ rsyncï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
6. æ›´æ–° checkpointï¼ˆè®°å½•æœ¬æ¬¡åŒæ­¥æ—¶é—´ï¼‰
7. æ¸…ç†æ—§æ•°æ®ï¼ˆè¶…è¿‡ retention_daysï¼‰
8. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯ï¼ˆä¼ è¾“é‡ã€æ–‡ä»¶æ•°ã€è€—æ—¶ï¼‰
```

### å‘½ä»¤ 2: æŸ¥çœ‹ç»Ÿè®¡

```bash
# æŸ¥çœ‹å•ä¸ª profile çš„æœ¬åœ°æ•°æ®ç»Ÿè®¡
quants-infra data-lake stats cex_ticks

# æŸ¥çœ‹æ‰€æœ‰ profiles
quants-infra data-lake stats --all

# è¾“å‡º JSON æ ¼å¼
quants-infra data-lake stats cex_ticks --format json
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```
Profile: cex_ticks
================================================================================
æœ¬åœ°è·¯å¾„:     /data/lake/cex_ticks
è¿œç¨‹æº:       ubuntu@10.0.0.11:/var/data/cex_tickers
æ•°æ®å¤§å°:     12.5 GB
æ–‡ä»¶æ•°:       3,421
æœ€æ—©æ•°æ®:     2024-10-29
æœ€æ–°æ•°æ®:     2024-11-28
ä¸Šæ¬¡åŒæ­¥:     2024-11-28 14:30:45 (2 å°æ—¶å‰)
ä¿ç•™ç­–ç•¥:     30 å¤©
```

### å‘½ä»¤ 3: æ¸…ç†æ—§æ•°æ®

```bash
# æ‰‹åŠ¨æ¸…ç†è¶…è¿‡ä¿ç•™æœŸçš„æ•°æ®
quants-infra data-lake cleanup cex_ticks

# æ¸…ç†æ‰€æœ‰ profiles
quants-infra data-lake cleanup --all

# å¹²è·‘ï¼ˆä»…æ˜¾ç¤ºå°†è¦åˆ é™¤çš„æ–‡ä»¶ï¼‰
quants-infra data-lake cleanup cex_ticks --dry-run
```

### å‘½ä»¤ 4: éªŒè¯é…ç½®

```bash
# éªŒè¯é…ç½®æ–‡ä»¶æ˜¯å¦æ­£ç¡®
quants-infra data-lake validate --config config/data_lake.yml
```

---

## ğŸ“‚ ç›®å½•ç»“æ„

### æœ¬åœ° Data Lake ç»“æ„

```
/data/lake/                              # root_dir
â”œâ”€â”€ .checkpoints/                        # checkpoint æ–‡ä»¶
â”‚   â”œâ”€â”€ cex_ticks.json
â”‚   â””â”€â”€ dex_candles.json
â”‚
â”œâ”€â”€ cex_ticks/                           # local_subdir (Profile 1)
â”‚   â”œâ”€â”€ gate_io_VIRTUAL-USDT_20241028/
â”‚   â”‚   â”œâ”€â”€ part_00001.parquet
â”‚   â”‚   â”œâ”€â”€ part_00002.parquet
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ gate_io_VIRTUAL-USDT_20241029/
â”‚   â”œâ”€â”€ gate_io_IRON-USDT_20241028/
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ dex_candles/                         # local_subdir (Profile 2)
    â”œâ”€â”€ uniswap_v3_WETH-USDC_20241028/
    â””â”€â”€ ...
```

### Checkpoint æ–‡ä»¶æ ¼å¼

```json
{
  "profile_name": "cex_ticks",
  "last_sync_time": "2024-11-28T14:30:45Z",
  "last_sync_status": "success",
  "files_transferred": 142,
  "bytes_transferred": 1234567890,
  "duration_seconds": 45.2,
  "errors": []
}
```

---

## ğŸ› ï¸ å®ç°æç¤º

### æ–‡ä»¶ç»“æ„

```
quants-infra/
â”œâ”€â”€ cli/
â”‚   â””â”€â”€ commands/
â”‚       â””â”€â”€ data_lake.py              # CLI å‘½ä»¤å®ç°
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ data_lake/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ manager.py                # DataLakeManager ä¸»ç±»
â”‚   â”‚   â”œâ”€â”€ syncer.py                 # RsyncSyncer åŒæ­¥å™¨
â”‚   â”‚   â”œâ”€â”€ cleaner.py                # RetentionCleaner æ¸…ç†å™¨
â”‚   â”‚   â”œâ”€â”€ checkpoint.py             # CheckpointManager
â”‚   â”‚   â””â”€â”€ stats.py                  # StatsCollector ç»Ÿè®¡
â”‚   â”‚
â”‚   â””â”€â”€ schemas/
â”‚       â””â”€â”€ data_lake_schema.py       # Pydantic schema
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ data_lake.yml                 # é…ç½®æ–‡ä»¶ç¤ºä¾‹
â”‚
â””â”€â”€ docs/
    â””â”€â”€ DATA_LAKE_MVP.md              # æœ¬æ–‡æ¡£
```

### æ ¸å¿ƒç±»è®¾è®¡

**DataLakeManager (core/data_lake/manager.py)**
```python
class DataLakeManager:
    """Data Lake ç®¡ç†å™¨"""

    def __init__(self, config_path: str):
        self.config = self._load_config(config_path)
        self.syncer = RsyncSyncer()
        self.cleaner = RetentionCleaner()
        self.checkpoint_mgr = CheckpointManager()
        self.stats = StatsCollector()

    def sync_profile(self, profile_name: str, dry_run: bool = False):
        """åŒæ­¥å•ä¸ª profile"""
        # 1. è·å– profile é…ç½®
        # 2. æ£€æŸ¥ checkpoint
        # 3. æ‰§è¡Œ rsync
        # 4. æ›´æ–° checkpoint
        # 5. æ¸…ç†æ—§æ•°æ®
        # 6. è¿”å›ç»Ÿè®¡ä¿¡æ¯
        pass

    def get_stats(self, profile_name: str):
        """è·å– profile ç»Ÿè®¡ä¿¡æ¯"""
        pass

    def cleanup(self, profile_name: str, dry_run: bool = False):
        """æ¸…ç†æ—§æ•°æ®"""
        pass
```

**RsyncSyncer (core/data_lake/syncer.py)**
```python
class RsyncSyncer:
    """rsync åŒæ­¥å™¨"""

    def sync(self, source_config: dict, local_path: str,
             rsync_args: str = "-az --partial --inplace",
             dry_run: bool = False) -> dict:
        """
        æ‰§è¡Œ rsync åŒæ­¥

        Returns:
            {
                'success': True,
                'files_transferred': 142,
                'bytes_transferred': 1234567890,
                'duration_seconds': 45.2,
                'stdout': '...',
                'stderr': ''
            }
        """
        # æ„å»º rsync å‘½ä»¤
        cmd = [
            'rsync',
            *rsync_args.split(),
            f"{source_config['user']}@{source_config['host']}:{source_config['remote_root']}/",
            f"{local_path}/"
        ]

        # å¦‚æœæœ‰ ssh_keyï¼Œæ·»åŠ  -e å‚æ•°
        if source_config.get('ssh_key'):
            ssh_cmd = f"ssh -i {source_config['ssh_key']} -p {source_config.get('port', 6677)}"
            cmd.insert(1, '-e')
            cmd.insert(2, ssh_cmd)

        # æ‰§è¡Œå‘½ä»¤
        result = subprocess.run(cmd, capture_output=True, text=True)

        # è§£æè¾“å‡ºï¼ˆæ–‡ä»¶æ•°ã€å­—èŠ‚æ•°ï¼‰
        return self._parse_rsync_output(result)
```

**RetentionCleaner (core/data_lake/cleaner.py)**
```python
class RetentionCleaner:
    """ä¿ç•™æœŸæ¸…ç†å™¨"""

    def cleanup_old_data(self, local_path: str, retention_days: int,
                        dry_run: bool = False) -> dict:
        """
        åˆ é™¤è¶…è¿‡ä¿ç•™æœŸçš„æ•°æ®

        å‡è®¾ç›®å½•ç»“æ„ï¼šexchange_symbol_YYYYMMDD/

        Returns:
            {
                'deleted_dirs': 5,
                'deleted_files': 142,
                'freed_bytes': 1234567890
            }
        """
        import os
        from datetime import datetime, timedelta

        cutoff_date = datetime.now() - timedelta(days=retention_days)

        # éå†ç›®å½•ï¼Œæ‰¾å‡ºè¶…è¿‡ä¿ç•™æœŸçš„æ–‡ä»¶å¤¹
        for dir_name in os.listdir(local_path):
            # æå–æ—¥æœŸï¼ˆå‡è®¾æ ¼å¼ï¼šexchange_symbol_YYYYMMDDï¼‰
            try:
                date_str = dir_name.split('_')[-1]  # æœ€åä¸€éƒ¨åˆ†æ˜¯æ—¥æœŸ
                dir_date = datetime.strptime(date_str, '%Y%m%d')

                if dir_date < cutoff_date:
                    # åˆ é™¤ç›®å½•
                    if not dry_run:
                        shutil.rmtree(os.path.join(local_path, dir_name))
            except:
                continue  # è·³è¿‡æ— æ³•è§£æçš„ç›®å½•
```

**CheckpointManager (core/data_lake/checkpoint.py)**
```python
class CheckpointManager:
    """Checkpoint ç®¡ç†å™¨"""

    def load_checkpoint(self, checkpoint_file: str) -> dict:
        """åŠ è½½ checkpoint"""
        if not os.path.exists(checkpoint_file):
            return {}
        with open(checkpoint_file) as f:
            return json.load(f)

    def save_checkpoint(self, checkpoint_file: str, data: dict):
        """ä¿å­˜ checkpoint"""
        os.makedirs(os.path.dirname(checkpoint_file), exist_ok=True)
        with open(checkpoint_file, 'w') as f:
            json.dump(data, f, indent=2)
```

### Pydantic Schema

```python
# core/schemas/data_lake_schema.py
from pydantic import BaseModel, Field, validator
from typing import Optional, Dict, List
from pathlib import Path

class SourceConfig(BaseModel):
    type: str = Field(..., regex="^ssh$")  # åªæ”¯æŒ ssh
    host: str
    port: int = 6677
    user: str
    ssh_key: Optional[str] = "~/.ssh/lightsail_key.pem"
    remote_root: str

    @validator('ssh_key')
    def expand_home(cls, v):
        return str(Path(v).expanduser()) if v else v

class ProfileConfig(BaseModel):
    enabled: bool = True
    source: SourceConfig
    local_subdir: str
    retention_days: int = 30
    rsync_args: str = "-az --partial --inplace"
    checkpoint_file: Optional[str] = None

class DataLakeConfig(BaseModel):
    root_dir: str
    checkpoint_dir: Optional[str] = None
    profiles: Dict[str, ProfileConfig]

    @validator('checkpoint_dir', pre=True, always=True)
    def default_checkpoint_dir(cls, v, values):
        if v is None:
            return f"{values.get('root_dir')}/.checkpoints"
        return v

    @validator('profiles')
    def set_checkpoint_files(cls, v, values):
        checkpoint_dir = values.get('checkpoint_dir')
        for name, profile in v.items():
            if profile.checkpoint_file is None:
                profile.checkpoint_file = f"{checkpoint_dir}/{name}.json"
        return v
```

### CLI å®ç°

```python
# cli/commands/data_lake.py
import click
from core.data_lake.manager import DataLakeManager

@click.group()
def data_lake():
    """Data Lake æ•°æ®åŒæ­¥å’Œç®¡ç†"""
    pass

@data_lake.command()
@click.argument('profile_name', required=False)
@click.option('--all', is_flag=True, help='åŒæ­¥æ‰€æœ‰å¯ç”¨çš„ profiles')
@click.option('--config', default='config/data_lake.yml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
@click.option('--dry-run', is_flag=True, help='ä»…æ˜¾ç¤ºå°†è¦æ‰§è¡Œçš„æ“ä½œ')
def sync(profile_name, all, config, dry_run):
    """åŒæ­¥æ•°æ®"""
    manager = DataLakeManager(config)

    if all:
        for name in manager.get_enabled_profiles():
            click.echo(f"Syncing profile: {name}")
            result = manager.sync_profile(name, dry_run=dry_run)
            _print_sync_result(result)
    elif profile_name:
        result = manager.sync_profile(profile_name, dry_run=dry_run)
        _print_sync_result(result)
    else:
        click.echo("Error: Specify --all or profile_name", err=True)

@data_lake.command()
@click.argument('profile_name', required=False)
@click.option('--all', is_flag=True)
@click.option('--config', default='config/data_lake.yml')
@click.option('--format', type=click.Choice(['table', 'json']), default='table')
def stats(profile_name, all, config, format):
    """æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯"""
    manager = DataLakeManager(config)

    # å®ç°ç»Ÿè®¡æ˜¾ç¤ºé€»è¾‘
    pass

@data_lake.command()
@click.argument('profile_name', required=False)
@click.option('--all', is_flag=True)
@click.option('--config', default='config/data_lake.yml')
@click.option('--dry-run', is_flag=True)
def cleanup(profile_name, all, config, dry_run):
    """æ¸…ç†æ—§æ•°æ®"""
    manager = DataLakeManager(config)

    # å®ç°æ¸…ç†é€»è¾‘
    pass

@data_lake.command()
@click.option('--config', default='config/data_lake.yml')
def validate(config):
    """éªŒè¯é…ç½®æ–‡ä»¶"""
    try:
        manager = DataLakeManager(config)
        click.echo("âœ“ é…ç½®æ–‡ä»¶éªŒè¯é€šè¿‡")
    except Exception as e:
        click.echo(f"âœ— é…ç½®æ–‡ä»¶éªŒè¯å¤±è´¥: {e}", err=True)
```

---

## ğŸ§ª æµ‹è¯•åœºæ™¯

### åœºæ™¯ 1: é¦–æ¬¡åŒæ­¥

```bash
# é…ç½®æ–‡ä»¶
cat config/data_lake.yml
# data_lake:
#   root_dir: /data/lake
#   profiles:
#     cex_ticks:
#       enabled: true
#       source:
#         host: 10.0.0.11
#         user: ubuntu
#         remote_root: /var/data/cex_tickers
#       local_subdir: cex_ticks
#       retention_days: 30

# é¦–æ¬¡åŒæ­¥
quants-infra data-lake sync cex_ticks

# é¢„æœŸè¾“å‡º
Syncing profile: cex_ticks
================================================================================
Remote: ubuntu@10.0.0.11:/var/data/cex_tickers
Local:  /data/lake/cex_ticks

Executing rsync...
  Files transferred: 1,421
  Bytes transferred: 5.2 GB
  Duration: 3m 42s

Updating checkpoint...
Cleaning up old data (retention: 30 days)...
  Deleted: 0 directories

âœ“ Sync completed successfully
```

### åœºæ™¯ 2: æ–­ç‚¹ç»­ä¼ 

```bash
# åŒæ­¥è¿‡ç¨‹ä¸­æ–­ï¼ˆç½‘ç»œä¸­æ–­ï¼‰
quants-infra data-lake sync cex_ticks
# ... ä¼ è¾“ä¸­ ...
# ^C (ç”¨æˆ·ä¸­æ–­)

# é‡æ–°åŒæ­¥ï¼ˆè‡ªåŠ¨ç»­ä¼ ï¼‰
quants-infra data-lake sync cex_ticks

# é¢„æœŸè¾“å‡º
Syncing profile: cex_ticks
================================================================================
Last sync: 2024-11-28 14:00:00 (incomplete)
Resuming from checkpoint...

Executing rsync...
  Files transferred: 42 (resumed)
  Bytes transferred: 320 MB
  Duration: 25s

âœ“ Sync completed successfully
```

### åœºæ™¯ 3: è‡ªåŠ¨æ¸…ç†æ—§æ•°æ®

```bash
# åŒæ­¥å¹¶è‡ªåŠ¨æ¸…ç†
quants-infra data-lake sync cex_ticks

# é¢„æœŸè¾“å‡ºï¼ˆå‡è®¾æœ‰è¶…è¿‡ 30 å¤©çš„æ•°æ®ï¼‰
...
Cleaning up old data (retention: 30 days)...
  Cutoff date: 2024-10-29
  Deleted: 5 directories
    - gate_io_VIRTUAL-USDT_20241028
    - gate_io_VIRTUAL-USDT_20241027
    - ...
  Freed space: 1.2 GB

âœ“ Sync completed successfully
```

---

## ğŸ“Š ç›‘æ§é›†æˆï¼ˆæœªæ¥æ‰©å±•ï¼‰

### Prometheus æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰

```python
# æœªæ¥å¯æ·»åŠ 
data_lake_sync_total{profile, status}               # åŒæ­¥æ¬¡æ•°
data_lake_sync_duration_seconds{profile}            # åŒæ­¥è€—æ—¶
data_lake_bytes_transferred_total{profile}          # ä¼ è¾“å­—èŠ‚æ•°
data_lake_files_transferred_total{profile}          # ä¼ è¾“æ–‡ä»¶æ•°
data_lake_cleanup_deleted_dirs_total{profile}       # æ¸…ç†ç›®å½•æ•°
data_lake_last_sync_timestamp{profile}              # æœ€ååŒæ­¥æ—¶é—´
```

---

## âœ… MVP å®Œæˆæ ‡å‡†

1. **é…ç½®é©±åŠ¨**: YAML é…ç½®æ–‡ä»¶æ”¯æŒå¤š profiles
2. **rsync åŒæ­¥**: æ”¯æŒ SSH è¿œç¨‹åŒæ­¥ + æ–­ç‚¹ç»­ä¼ 
3. **checkpoint**: è®°å½•åŒæ­¥çŠ¶æ€ï¼Œæ”¯æŒæ¢å¤
4. **retention**: è‡ªåŠ¨æ¸…ç†è¶…è¿‡ä¿ç•™æœŸçš„æ•°æ®
5. **CLI å‘½ä»¤**: `sync`, `stats`, `cleanup`, `validate`
6. **é”™è¯¯å¤„ç†**: ç½‘ç»œä¸­æ–­ã€æƒé™é”™è¯¯ã€ç£ç›˜ç©ºé—´ä¸è¶³

**ä¸éœ€è¦çš„åŠŸèƒ½ï¼ˆMVP é˜¶æ®µï¼‰ï¼š**
- âŒ æ•°æ®åˆå¹¶/è½¬æ¢
- âŒ å®æ—¶åŒæ­¥ï¼ˆcron job è¶³å¤Ÿï¼‰
- âŒ åˆ†å¸ƒå¼éƒ¨ç½²
- âŒ æ•°æ®éªŒè¯/æ ¡éªŒå’Œ
- âŒ Web UI

---

## ğŸš€ ä½¿ç”¨æµç¨‹

### Step 1: åˆ›å»ºé…ç½®æ–‡ä»¶

```bash
cd quants-infra
cp config/data_lake.example.yml config/data_lake.yml
vim config/data_lake.yml
# ä¿®æ”¹ hostã€userã€remote_rootã€root_dir ç­‰å‚æ•°
```

### Step 2: éªŒè¯é…ç½®

```bash
quants-infra data-lake validate
# âœ“ é…ç½®æ–‡ä»¶éªŒè¯é€šè¿‡
```

### Step 3: é¦–æ¬¡åŒæ­¥

```bash
quants-infra data-lake sync cex_ticks
# ä¼ è¾“æ•°æ®...
```

### Step 4: è®¾ç½®å®šæ—¶åŒæ­¥ï¼ˆcronï¼‰

```bash
# ç¼–è¾‘ crontab
crontab -e

# æ¯å°æ—¶åŒæ­¥ä¸€æ¬¡
0 * * * * cd /path/to/quants-infra && quants-infra data-lake sync --all >> /var/log/data-lake-sync.log 2>&1
```

### Step 5: æŸ¥çœ‹ç»Ÿè®¡

```bash
quants-infra data-lake stats cex_ticks
```

---

## ğŸ“ æ€»ç»“

**è¿™ä¸ª MVP ç‰ˆæœ¬ï¼š**
- âœ… æç®€ï¼ˆ<500 è¡Œä»£ç ï¼‰
- âœ… å¯é ï¼ˆrsync + checkpointï¼‰
- âœ… é…ç½®é©±åŠ¨ï¼ˆYAMLï¼‰
- âœ… æ˜“æ‰©å±•ï¼ˆPydantic schemaï¼‰

**é€‚åˆç›´æ¥äº¤ç»™ Cursor/Claude Code ç”Ÿæˆå®ç°ã€‚**

---

**æœ€åæ›´æ–°**: 2025-11-28
**ä½œè€…**: Alice
**ç‰ˆæœ¬**: MVP v1.0
